{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiastornquist/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from lstm_vae import create_lstm_vae, inference\n",
    "from train import get_text_data\n",
    "import keras\n",
    "import sys, time\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/mathiastornquist/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes file to local data filder\n",
    "print(webtext.fileids())\n",
    "raw_text = webtext.raw(webtext.fileids()[5])\n",
    "listan = [sentence.replace('\\t','').replace('*','').replace('\\n','').strip() for sentence in re.split(\"[.!?]+\", raw_text) if len(sentence.split())>3]\n",
    "with open('data/wine.txt','w') as f:\n",
    "    for sentence in listan:\n",
    "        f.write(sentence+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2263\n",
      "Number of unique input tokens: 3733\n",
      "Max sequence length for inputs: 48\n",
      "Training model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 3733)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 353)          5770844     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 191)          67614       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 191)          67614       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 191)          0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 3733)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 353)          67776       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 353),  5770844     input_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 3733)   1321482     lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 13,066,174\n",
      "Trainable params: 13,066,174\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/60\n",
      "2263/2263 [==============================] - 140s 62ms/step - loss: 1.4901 - xent_loss: 1.4862 - kl_loss: 0.0040\n",
      "Epoch 2/60\n",
      "2263/2263 [==============================] - 139s 61ms/step - loss: 1.3604 - xent_loss: 1.3595 - kl_loss: 9.2749e-04\n",
      "Epoch 3/60\n",
      "2263/2263 [==============================] - 135s 60ms/step - loss: 1.3405 - xent_loss: 1.3398 - kl_loss: 6.8707e-04\n",
      "Epoch 4/60\n",
      "2263/2263 [==============================] - 128s 57ms/step - loss: 1.3263 - xent_loss: 1.3257 - kl_loss: 6.2760e-04\n",
      "Epoch 5/60\n",
      "2263/2263 [==============================] - 124s 55ms/step - loss: 1.3137 - xent_loss: 1.3132 - kl_loss: 5.4854e-04\n",
      "Epoch 6/60\n",
      "2263/2263 [==============================] - 124s 55ms/step - loss: 1.3001 - xent_loss: 1.2997 - kl_loss: 4.4989e-04\n",
      "Epoch 7/60\n",
      "2263/2263 [==============================] - 125s 55ms/step - loss: 1.2846 - xent_loss: 1.2843 - kl_loss: 3.1612e-04\n",
      "Epoch 8/60\n",
      " 448/2263 [====>.........................] - ETA: 1:44 - loss: 1.2087 - xent_loss: 1.2084 - kl_loss: 2.8006e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a4ff12b46603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'print_default'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a4ff12b46603>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mcsv_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_vae.log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_decoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main(params):\n",
    "    \n",
    "    num_samples = int(params['num_samples'])\n",
    "    data_path = \"data/\" + params['dataset']\n",
    "    dataname = params['dataset'].split('.')[0]\n",
    "    \n",
    "    batch_size = int(params['batch_size'])\n",
    "    latent_dim = int(params['latent_dim'])\n",
    "    intermediate_dim = int(params['intermediate_dim'])\n",
    "    epochs = int(params['epochs'])\n",
    "    \n",
    "    train = int(params['train'])\n",
    "    save = int(params['save'])\n",
    "    load = int(params['load'])\n",
    "    \n",
    "    print_default = int(params['print_default'])\n",
    "    \n",
    "   \n",
    "    timesteps_max, enc_tokens, characters, char2id, id2char, x, x_decoder = get_text_data(num_samples=num_samples,\n",
    "                                                                                          data_path=data_path)\n",
    "\n",
    "    \n",
    "    input_dim = x.shape[-1]\n",
    "    timesteps = x.shape[-2]\n",
    "\n",
    "        \n",
    "    if load:\n",
    "        print(\"Loading model ... \")\n",
    "        \n",
    "        #vae = keras.models.load_model(\"models/vae_{}_{}.h5\".format(dataname, num_samples))\n",
    "        enc = keras.models.load_model(\"models/encoder_{}_{}.h5\".format(dataname, num_samples))\n",
    "        gen = keras.models.load_model(\"models/generator_{}_{}.h5\".format(dataname, num_samples))\n",
    "        stepper = keras.models.load_model(\"models/stepper_{}_{}.h5\".format(dataname, num_samples))\n",
    "    \n",
    "    if train:\n",
    "        print(\"Training model...\")\n",
    "        \n",
    "        vae, enc, gen, stepper = create_lstm_vae(input_dim,\n",
    "                                             batch_size=batch_size,\n",
    "                                             intermediate_dim=intermediate_dim,\n",
    "                                             latent_dim=latent_dim)\n",
    "        \n",
    "        csv_logger = CSVLogger('training_vae.log', separator=',', append=False)\n",
    "        vae.fit([x, x_decoder], x, epochs=epochs, verbose=1, callbacks=[csv_logger])\n",
    "        \n",
    "        if save:\n",
    "            print(\"Saving model ... \")\n",
    "            \n",
    "            vae.save(\"models/vae_{}_{}.h5\".format(dataname, num_samples))\n",
    "            enc.save(\"models/encoder_{}_{}.h5\".format(dataname, num_samples))\n",
    "            gen.save(\"models/generator_{}_{}.h5\".format(dataname, num_samples))\n",
    "            stepper.save(\"models/stepper_{}_{}.h5\".format(dataname, num_samples))\n",
    "\n",
    "    def decode(s, start_char = \"\\t\"):\n",
    "        return inference.decode_sequence(s, gen, stepper, input_dim, char2id, id2char, timesteps_max, start_char = start_char)\n",
    "\n",
    "    def continue_seq(x_start, states_value, h0 = False, sampling = False):\n",
    "        return inference.continue_sequence(x_start, states_value, h0, sampling, gen, stepper, input_dim, char2id, id2char, timesteps_max)\n",
    "    \n",
    "    if print_default:\n",
    "\n",
    "        for _ in range(6):\n",
    "            id_from = np.random.randint(0, x.shape[0] - 1)\n",
    "            id_sentence = np.random.randint(0, x.shape[0] - 1)\n",
    "\n",
    "            n_words = np.sum(x[id_sentence])\n",
    "            n_kept = np.random.randint(n_words//2, n_words-1)\n",
    "\n",
    "            new_x = np.zeros((x[id_sentence].shape))\n",
    "            new_x[:n_kept,:] = x[id_sentence,:n_kept,:]\n",
    "\n",
    "            m_new, std_new = enc.predict([[x[id_from]]])\n",
    "            h_new = np.random.normal(size=(latent_dim,))\n",
    "            states_new = m_new + std_new * h_new\n",
    "\n",
    "            print(\"==  \\t\", \" \".join([id2char[j] for j in np.argmax(new_x[:n_kept], axis=1)]), \" ... \\t\\t ==\")\n",
    "\n",
    "            print(\"\\t...\\t\", continue_seq(new_x, states_new))\n",
    "            print(\"\\t...\\t\", continue_seq(new_x, states_new, h0 = True))\n",
    "\n",
    "            print(\"\\t...\\t\", continue_seq(new_x, states_new, sampling = True))\n",
    "            print(\"\\t...\\t\", continue_seq(new_x, states_new, h0 = True, sampling = True))\n",
    "\n",
    "\n",
    "            print(\"==  \\t\", \" \".join([id2char[j] for j in np.argmax(x[id_sentence], axis=1)]), \"==\")\n",
    "            \n",
    "            \n",
    "raw_text = webtext.raw(webtext.fileids()[5])\n",
    "listan = [sentence.replace('\\t','').replace('*','').replace('\\n','').strip() for sentence in re.split(\"[.!?]+\", raw_text) if len(sentence.split())>3]\n",
    "\n",
    "\n",
    "params = {}\n",
    "params['batch_size'] = 1\n",
    "params['latent_dim'] = 191\n",
    "params['intermediate_dim'] = 353\n",
    "params['epochs'] = 60\n",
    "params['verbose'] = 1\n",
    "params['num_samples'] = len(listan)\n",
    "params['data_type'] = \"text\"\n",
    "params['dataset'] = webtext.fileids()[5] #'wine.txt'\n",
    "params['train'] = 1\n",
    "params['save'] = 1\n",
    "params['load'] = 0\n",
    "params['print_default'] = 1\n",
    "\n",
    "main(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loads the wine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "print(webtext.fileids())\n",
    "raw_text = webtext.raw(webtext.fileids()[5])\n",
    "listan = [sentence.replace('\\t','').replace('*','').replace('\\n','').strip() for sentence in re.split(\"[.!?]+\", raw_text) if len(sentence.split())>3]\n",
    "params = {}\n",
    "params['batch_size'] = 1\n",
    "params['latent_dim'] = 191\n",
    "params['intermediate_dim'] = 353\n",
    "params['epochs'] = 40\n",
    "params['verbose'] = 1\n",
    "params['num_samples'] = len(listan)\n",
    "params['data_type'] = \"text\"\n",
    "params['dataset'] = webtext.fileids()[5] # 'wine.txt'\n",
    "params['train'] = 0\n",
    "params['save'] = 0\n",
    "params['load'] = 1\n",
    "params['print_default'] = 1\n",
    "\n",
    "main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
