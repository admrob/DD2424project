{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import sys, time\n",
    "from keras.callbacks import CSVLogger\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "def get_mnist_data(num_samples=1000):\n",
    "            \n",
    "    # load data\n",
    "    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    n_inputs, height, max_length = X_train.shape\n",
    "    encoder_input_data = np.zeros((n_inputs, max_length + 1, height), dtype=\"float32\")\n",
    "    decoder_input_data = np.zeros((n_inputs, max_length + 1, height), dtype=\"float32\")\n",
    "    encoder_input_data[:,:max_length,:] = np.swapaxes(X_train, 1, 2).copy()/255\n",
    "    decoder_input_data[:,1:,:] = np.swapaxes(X_train, 1, 2).copy()/255\n",
    "\n",
    "    return max_length + 1, height, encoder_input_data[:num_samples,:,:], decoder_input_data[:num_samples,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/4\n",
      "1000/1000 [==============================] - 19s 19ms/step - loss: 0.4197 - bc_loss: 0.4197 - kl_loss: 1.7329e-04\n",
      "Epoch 2/4\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3923 - bc_loss: 0.3923 - kl_loss: 1.6630e-04\n",
      "Epoch 3/4\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3743 - bc_loss: 0.3743 - kl_loss: 1.5626e-04\n",
      "Epoch 4/4\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3632 - bc_loss: 0.3632 - kl_loss: 2.0194e-04\n",
      "[0.4196508240699768, 0.39225774812698366, 0.3743474795818329, 0.3631816101074219]\n",
      "1000/1000 [==============================] - 7s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1375723991394043"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_with(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with(intermediate_dim, latent_dim, prints=False):\n",
    "    num_samples = 500\n",
    "    timesteps_max, enc_tokens, x, x_decoder = get_mnist_data(num_samples=num_samples)\n",
    "\n",
    "    print(x.shape, \"Creating model...\")\n",
    "\n",
    "    input_dim = x.shape[-1]\n",
    "    timesteps = x.shape[-2]\n",
    "    batch_size = 1\n",
    "    #latent_dim = 191\n",
    "    #intermediate_dim = 353\n",
    "    latent_dim = math.ceil(latent_dim)\n",
    "    intermediate_dim = math.ceil(intermediate_dim)\n",
    "    epochs = 1\n",
    "\n",
    "    vae, enc, gen, stepper = create_lstm_vae(input_dim,\n",
    "                                             batch_size=batch_size,\n",
    "                                             intermediate_dim=intermediate_dim,\n",
    "                                             latent_dim=latent_dim)\n",
    "    print(\"Training model...\")\n",
    "\n",
    "    hist = vae.fit([x, x_decoder], x, epochs=epochs, verbose=1)\n",
    "    \n",
    "    \n",
    "    if not prints:\n",
    "        return -hist.history['loss'][-1]\n",
    "    else:\n",
    "        \n",
    "        def decode(s, start_char = \"\\t\"):\n",
    "            return decode_sequence(s, gen, stepper, input_dim, timesteps_max)\n",
    "    \n",
    "        for _ in range(5):\n",
    "\n",
    "            id_from = np.random.randint(0, x.shape[0] - 1)\n",
    "            id_to = np.random.randint(0, x.shape[0] - 1)\n",
    "\n",
    "            m_from, std_from = enc.predict([[x[id_from]]])\n",
    "            m_to, std_to = enc.predict([[x[id_to]]])\n",
    "\n",
    "            seq_from = np.random.normal(size=(latent_dim,))\n",
    "            seq_from = m_from #+ std_from * seq_from\n",
    "\n",
    "            seq_to = np.random.normal(size=(latent_dim,))\n",
    "            seq_to = m_to #+ std_to * seq_to\n",
    "\n",
    "\n",
    "\n",
    "            print(\"== from \\t ==\")\n",
    "            plt.imshow(x[id_from].T, cmap='Greys',  interpolation='nearest')\n",
    "            plt.grid(False)\n",
    "            plt.show()\n",
    "\n",
    "            for v in np.linspace(0, 1, 7):\n",
    "                print(\"%.2f\\t\" % (1 - v))\n",
    "                plt.imshow(decode(v * seq_to + (1 - v) * seq_from).T, cmap='Greys',  interpolation='nearest')\n",
    "                plt.grid(False)\n",
    "                plt.show()\n",
    "                \n",
    "from functools import partial\n",
    "\n",
    "verbose = 1\n",
    "fit_with_partial = partial(fit_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | interm... | latent... |\n",
      "-------------------------------------------------\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 61s 123ms/step - loss: 0.4074 - bc_loss: 0.4074 - kl_loss: 1.0784e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 28s 56ms/step - loss: 0.3848 - bc_loss: 0.3848 - kl_loss: 1.2943e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 28s 56ms/step - loss: 0.3699 - bc_loss: 0.3699 - kl_loss: 2.8544e-04\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.3699  \u001b[0m | \u001b[0m 1.242e+0\u001b[0m | \u001b[0m 1.636e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 40s 81ms/step - loss: 0.4071 - bc_loss: 0.4071 - kl_loss: 1.8122e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.3857 - bc_loss: 0.3857 - kl_loss: 1.3494e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.3707 - bc_loss: 0.3707 - kl_loss: 3.4567e-04\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.3707  \u001b[0m | \u001b[0m 700.1   \u001b[0m | \u001b[0m 1.093e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 44s 89ms/step - loss: 0.4065 - bc_loss: 0.4065 - kl_loss: 1.3783e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.3845 - bc_loss: 0.3845 - kl_loss: 1.5857e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.3701 - bc_loss: 0.3701 - kl_loss: 3.0437e-04\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.3701  \u001b[0m | \u001b[0m 890.8   \u001b[0m | \u001b[0m 820.0   \u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 46s 92ms/step - loss: 0.4079 - bc_loss: 0.4079 - kl_loss: 1.4390e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 14s 28ms/step - loss: 0.3863 - bc_loss: 0.3863 - kl_loss: 1.5884e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 14s 28ms/step - loss: 0.3718 - bc_loss: 0.3718 - kl_loss: 3.6690e-04\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.3718  \u001b[0m | \u001b[0m 942.1   \u001b[0m | \u001b[0m 1.149e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 61s 122ms/step - loss: 0.4089 - bc_loss: 0.4089 - kl_loss: 1.6132e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 25s 50ms/step - loss: 0.3869 - bc_loss: 0.3869 - kl_loss: 1.4720e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 25s 51ms/step - loss: 0.3743 - bc_loss: 0.3743 - kl_loss: 1.6027e-04\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.3743  \u001b[0m | \u001b[0m 1.216e+0\u001b[0m | \u001b[0m 1.4e+03 \u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.4051 - bc_loss: 0.4051 - kl_loss: 1.1450e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 28s 57ms/step - loss: 0.3890 - bc_loss: 0.3890 - kl_loss: 1.1667e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 28s 56ms/step - loss: 0.3758 - bc_loss: 0.3758 - kl_loss: 1.6651e-04\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.3758  \u001b[0m | \u001b[0m 1.245e+0\u001b[0m | \u001b[0m 1.591e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 52s 104ms/step - loss: 0.4065 - bc_loss: 0.4065 - kl_loss: 1.1045e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 16s 32ms/step - loss: 0.3877 - bc_loss: 0.3877 - kl_loss: 1.4793e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 16s 31ms/step - loss: 0.3741 - bc_loss: 0.3741 - kl_loss: 2.1128e-04\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.3741  \u001b[0m | \u001b[0m 965.8   \u001b[0m | \u001b[0m 1.842e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 46s 92ms/step - loss: 0.4094 - bc_loss: 0.4094 - kl_loss: 1.2186e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.3890 - bc_loss: 0.3890 - kl_loss: 9.8120e-05\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.3739 - bc_loss: 0.3739 - kl_loss: 3.3690e-04\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.3739  \u001b[0m | \u001b[0m 735.6   \u001b[0m | \u001b[0m 1.572e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 66s 133ms/step - loss: 0.4059 - bc_loss: 0.4059 - kl_loss: 1.6443e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 27s 53ms/step - loss: 0.3879 - bc_loss: 0.3879 - kl_loss: 1.3870e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 27s 55ms/step - loss: 0.3717 - bc_loss: 0.3717 - kl_loss: 2.1312e-04\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.3717  \u001b[0m | \u001b[0m 1.242e+0\u001b[0m | \u001b[0m 1.426e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 48s 97ms/step - loss: 0.4082 - bc_loss: 0.4082 - kl_loss: 1.5045e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 12s 25ms/step - loss: 0.3868 - bc_loss: 0.3868 - kl_loss: 1.1481e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 13s 25ms/step - loss: 0.3747 - bc_loss: 0.3747 - kl_loss: 1.5263e-04\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.3747  \u001b[0m | \u001b[0m 882.5   \u001b[0m | \u001b[0m 957.5   \u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 0.4060 - bc_loss: 0.4060 - kl_loss: 8.2656e-05\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 66s 131ms/step - loss: 0.3816 - bc_loss: 0.3816 - kl_loss: 2.9392e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 66s 133ms/step - loss: 0.3786 - bc_loss: 0.3786 - kl_loss: 1.9645e-04\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.3786  \u001b[0m | \u001b[0m 1.998e+0\u001b[0m | \u001b[0m 700.3   \u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 119s 237ms/step - loss: 0.4072 - bc_loss: 0.4072 - kl_loss: 1.3281e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 71s 143ms/step - loss: 0.3835 - bc_loss: 0.3835 - kl_loss: 1.8036e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.3704 - bc_loss: 0.3704 - kl_loss: 2.6549e-04\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.3704  \u001b[0m | \u001b[0m 2e+03   \u001b[0m | \u001b[0m 1.995e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 71s 143ms/step - loss: 0.4093 - bc_loss: 0.4093 - kl_loss: 1.8331e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 8s 15ms/step - loss: 0.3867 - bc_loss: 0.3867 - kl_loss: 1.6180e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.3725 - bc_loss: 0.3725 - kl_loss: 2.3566e-04\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.3725  \u001b[0m | \u001b[0m 701.0   \u001b[0m | \u001b[0m 701.3   \u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 130s 261ms/step - loss: 0.4105 - bc_loss: 0.4105 - kl_loss: 7.3322e-05\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 70s 140ms/step - loss: 0.3857 - bc_loss: 0.3857 - kl_loss: 1.9304e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 67s 133ms/step - loss: 0.3707 - bc_loss: 0.3707 - kl_loss: 2.9542e-04\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.3707  \u001b[0m | \u001b[0m 1.994e+0\u001b[0m | \u001b[0m 1.999e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 141s 282ms/step - loss: 0.4081 - bc_loss: 0.4081 - kl_loss: 1.3163e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 67s 134ms/step - loss: 0.3858 - bc_loss: 0.3858 - kl_loss: 2.5342e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 66s 133ms/step - loss: 0.3712 - bc_loss: 0.3712 - kl_loss: 2.5859e-04\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.3712  \u001b[0m | \u001b[0m 1.998e+0\u001b[0m | \u001b[0m 1.998e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 155s 310ms/step - loss: 0.4092 - bc_loss: 0.4092 - kl_loss: 7.7116e-05\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 71s 142ms/step - loss: 0.3846 - bc_loss: 0.3846 - kl_loss: 2.2221e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 69s 139ms/step - loss: 0.3732 - bc_loss: 0.3732 - kl_loss: 1.5110e-04\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.3732  \u001b[0m | \u001b[0m 1.999e+0\u001b[0m | \u001b[0m 1.999e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 94s 188ms/step - loss: 0.4086 - bc_loss: 0.4086 - kl_loss: 1.7674e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.3886 - bc_loss: 0.3886 - kl_loss: 1.7703e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.3751 - bc_loss: 0.3751 - kl_loss: 1.5127e-04\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.3751  \u001b[0m | \u001b[0m 706.7   \u001b[0m | \u001b[0m 703.0   \u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 175s 350ms/step - loss: 0.4106 - bc_loss: 0.4106 - kl_loss: 7.8609e-05\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 68s 136ms/step - loss: 0.3892 - bc_loss: 0.3892 - kl_loss: 2.8406e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 67s 135ms/step - loss: 0.3733 - bc_loss: 0.3733 - kl_loss: 2.2265e-04\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.3733  \u001b[0m | \u001b[0m 1.997e+0\u001b[0m | \u001b[0m 1.994e+0\u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 149s 298ms/step - loss: 0.4051 - bc_loss: 0.4051 - kl_loss: 1.7700e-04\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 40s 81ms/step - loss: 0.3869 - bc_loss: 0.3869 - kl_loss: 3.3731e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 40s 81ms/step - loss: 0.3726 - bc_loss: 0.3726 - kl_loss: 2.3297e-04\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.3726  \u001b[0m | \u001b[0m 1.533e+0\u001b[0m | \u001b[0m 2e+03   \u001b[0m |\n",
      "(500, 29, 28) Creating model...\n",
      "Training model...\n",
      "Epoch 1/3\n",
      "500/500 [==============================] - 183s 366ms/step - loss: 0.4075 - bc_loss: 0.4075 - kl_loss: 8.7103e-05\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 63s 126ms/step - loss: 0.3851 - bc_loss: 0.3851 - kl_loss: 1.3395e-04\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 62s 125ms/step - loss: 0.3711 - bc_loss: 0.3711 - kl_loss: 1.4698e-04\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.3711  \u001b[0m | \u001b[0m 2e+03   \u001b[0m | \u001b[0m 1.999e+0\u001b[0m |\n",
      "=================================================\n",
      "Iteration 0: \n",
      "\t{'target': -0.3698778395652771, 'params': {'intermediate_dim': 1242.1286061133462, 'latent_dim': 1636.4218414748057}}\n",
      "Iteration 1: \n",
      "\t{'target': -0.3707469313144684, 'params': {'intermediate_dim': 700.1486872625484, 'latent_dim': 1093.0323444213918}}\n",
      "Iteration 2: \n",
      "\t{'target': -0.37009035181999206, 'params': {'intermediate_dim': 890.7826580622469, 'latent_dim': 820.0401731994372}}\n",
      "Iteration 3: \n",
      "\t{'target': -0.3718418951034546, 'params': {'intermediate_dim': 942.1382747909722, 'latent_dim': 1149.228945155962}}\n",
      "Iteration 4: \n",
      "\t{'target': -0.374343332529068, 'params': {'intermediate_dim': 1215.797716499871, 'latent_dim': 1400.461754204364}}\n",
      "Iteration 5: \n",
      "\t{'target': -0.37575996589660643, 'params': {'intermediate_dim': 1244.9528687242832, 'latent_dim': 1590.7853505157873}}\n",
      "Iteration 6: \n",
      "\t{'target': -0.37410937213897705, 'params': {'intermediate_dim': 965.7879246509726, 'latent_dim': 1841.552667308229}}\n",
      "Iteration 7: \n",
      "\t{'target': -0.37386380434036254, 'params': {'intermediate_dim': 735.603871157304, 'latent_dim': 1571.607763231923}}\n",
      "Iteration 8: \n",
      "\t{'target': -0.3716656243801117, 'params': {'intermediate_dim': 1242.496243077265, 'latent_dim': 1426.2967769794773}}\n",
      "Iteration 9: \n",
      "\t{'target': -0.37467357444763183, 'params': {'intermediate_dim': 882.5030201738039, 'latent_dim': 957.5319358103425}}\n",
      "Iteration 10: \n",
      "\t{'target': -0.378600515127182, 'params': {'intermediate_dim': 1997.574765303353, 'latent_dim': 700.3420829958808}}\n",
      "Iteration 11: \n",
      "\t{'target': -0.370366828918457, 'params': {'intermediate_dim': 1999.94288095221, 'latent_dim': 1994.7714074247767}}\n",
      "Iteration 12: \n",
      "\t{'target': -0.37246668004989625, 'params': {'intermediate_dim': 700.9731609540052, 'latent_dim': 701.2640777230657}}\n",
      "Iteration 13: \n",
      "\t{'target': -0.3706930499076843, 'params': {'intermediate_dim': 1994.4998474373378, 'latent_dim': 1998.9514212645483}}\n",
      "Iteration 14: \n",
      "\t{'target': -0.37124485802650453, 'params': {'intermediate_dim': 1997.5088233957767, 'latent_dim': 1998.0671257009328}}\n",
      "Iteration 15: \n",
      "\t{'target': -0.3732314829826355, 'params': {'intermediate_dim': 1998.845159390258, 'latent_dim': 1998.6488731108848}}\n",
      "Iteration 16: \n",
      "\t{'target': -0.3750878162384033, 'params': {'intermediate_dim': 706.7132150421755, 'latent_dim': 702.951723033625}}\n",
      "Iteration 17: \n",
      "\t{'target': -0.3733467998504639, 'params': {'intermediate_dim': 1997.2519916228205, 'latent_dim': 1994.242308819514}}\n",
      "Iteration 18: \n",
      "\t{'target': -0.37257598733901975, 'params': {'intermediate_dim': 1533.4207768377973, 'latent_dim': 1999.6463476194788}}\n",
      "Iteration 19: \n",
      "\t{'target': -0.3710664930343628, 'params': {'intermediate_dim': 1999.674922123776, 'latent_dim': 1998.7459753070489}}\n",
      "{'target': -0.3698778395652771, 'params': {'intermediate_dim': 1242.1286061133462, 'latent_dim': 1636.4218414748057}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'intermediate_dim':(100,5000), 'latent_dim':(100,5000)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=50, n_iter=50,)\n",
    "\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.layers import Input, LSTM\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def create_lstm_vae(input_dim,\n",
    "                    batch_size,  # we need it for sampling\n",
    "                    intermediate_dim,\n",
    "                    latent_dim):\n",
    "    \"\"\"\n",
    "    Creates an LSTM Variational Autoencoder (VAE).\n",
    "\n",
    "    # Arguments\n",
    "        input_dim: int.\n",
    "        batch_size: int.\n",
    "        intermediate_dim: int, output shape of LSTM.\n",
    "        latent_dim: int, latent z-layer shape.\n",
    "        epsilon_std: float, z-layer sigma.\n",
    "\n",
    "\n",
    "    # References\n",
    "        - [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "        - [Generating sentences from a continuous space](https://arxiv.org/abs/1511.06349)\n",
    "    \"\"\"\n",
    "    x = Input(shape=(None, input_dim,))\n",
    "\n",
    "    # LSTM encoding\n",
    "    h = LSTM(units=intermediate_dim)(x)\n",
    "\n",
    "    # VAE Z layer\n",
    "    z_mean = Dense(units=latent_dim)(h)\n",
    "    z_log_sigma = Dense(units=latent_dim)(h)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.0)\n",
    "        return z_mean + z_log_sigma * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    # so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "    z_reweighting = Dense(units=intermediate_dim, activation=\"linear\")\n",
    "    z_reweighted = z_reweighting(z)\n",
    "\n",
    "    # \"next-word\" data for prediction\n",
    "    decoder_words_input = Input(shape=(None, input_dim,))\n",
    "\n",
    "    # decoded LSTM layer\n",
    "    decoder_h = LSTM(intermediate_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "    # todo: not sure if this initialization is correct\n",
    "    h_decoded, _, _ = decoder_h(decoder_words_input, initial_state=[z_reweighted, z_reweighted])\n",
    "    decoder_dense = TimeDistributed(Dense(input_dim, activation=\"softmax\"))\n",
    "    decoded_onehot = decoder_dense(h_decoded)\n",
    "\n",
    "    # end-to-end autoencoder\n",
    "    vae = Model([x, decoder_words_input], decoded_onehot)\n",
    "\n",
    "    # encoder, from inputs to latent space\n",
    "    encoder = Model(x, [z_mean, z_log_sigma])\n",
    "\n",
    "    # generator, from latent space to reconstructed inputs -- for inference's first step\n",
    "    decoder_state_input = Input(shape=(latent_dim,))\n",
    "    _z_rewighted = z_reweighting(decoder_state_input)\n",
    "    _h_decoded, _decoded_h, _decoded_c = decoder_h(decoder_words_input, initial_state=[_z_rewighted, _z_rewighted])\n",
    "    _decoded_onehot = decoder_dense(_h_decoded)\n",
    "    generator = Model([decoder_words_input, decoder_state_input], [_decoded_onehot, _decoded_h, _decoded_c])\n",
    "\n",
    "    # RNN for inference\n",
    "    input_h = Input(shape=(intermediate_dim,))\n",
    "    input_c = Input(shape=(intermediate_dim,))\n",
    "    __h_decoded, __decoded_h, __decoded_c = decoder_h(decoder_words_input, initial_state=[input_h, input_c])\n",
    "    __decoded_onehot = decoder_dense(__h_decoded)\n",
    "    stepper = Model([decoder_words_input, input_h, input_c], [__decoded_onehot, __decoded_h, __decoded_c])\n",
    "\n",
    "    def vae_loss(x, x_decoded_onehot):\n",
    "        xent_loss = objectives.categorical_crossentropy(x, x_decoded_onehot)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "        loss = xent_loss + kl_loss\n",
    "        return loss\n",
    "    \n",
    "    def xent_loss(x, x_decoded_onehot):\n",
    "        xent_loss = objectives.categorical_crossentropy(x, x_decoded_onehot)\n",
    "        return xent_loss\n",
    "    \n",
    "\n",
    "    def kl_loss(x, x_decoded_onehot):\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "        return kl_loss\n",
    "    \n",
    "    def bc_loss(x, x_decoded_onehot):\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "        bc_loss = objectives.binary_crossentropy(x, x_decoded_onehot)\n",
    "        return bc_loss+kl_loss\n",
    "    \n",
    "    get_custom_objects().update({\"bc_loss\": bc_loss, 'xent_loss': xent_loss, 'kl_loss':kl_loss})\n",
    "\n",
    "    vae.compile(optimizer=\"adam\", loss=bc_loss, metrics = [bc_loss, kl_loss])\n",
    "    #vae.summary()\n",
    "\n",
    "    return vae, encoder, generator, stepper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def decode_sequence(states_value, decoder_adapter_model, rnn_decoder_model, num_decoder_tokens, max_seq_length):\n",
    "    \"\"\"\n",
    "    Decoding adapted from this example:\n",
    "    https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "    :param states_value:\n",
    "    :param decoder_adapter_model: reads text representation, makes the first prediction, yields states after the first RNN's step\n",
    "    :param rnn_decoder_model: reads previous states and makes one RNN step\n",
    "    :param num_decoder_tokens:\n",
    "    :param token2id: dict mapping words to ids\n",
    "    :param id2token: dict mapping ids to words\n",
    "    :param max_seq_length: the maximum length of the sequence\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "\n",
    "    # sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1)\n",
    "    stop_condition = False\n",
    "\n",
    "    decoded_sentence = np.zeros((max_seq_length, num_decoder_tokens))\n",
    "\n",
    "    first_time = True\n",
    "    h, c = None, None\n",
    "        \n",
    "    t = 0\n",
    "    while not stop_condition:\n",
    "\n",
    "        if first_time:\n",
    "            # feeding in states sampled with the mean and std provided by encoder\n",
    "            # and getting current LSTM states to feed in to the decoder at the next step\n",
    "            output_tokens, h, c = decoder_adapter_model.predict([target_seq, states_value])\n",
    "            first_time = False\n",
    "        else:\n",
    "            # reading output token\n",
    "            output_tokens, h, c = rnn_decoder_model.predict([target_seq, h, c])\n",
    "\n",
    "        # sample a token\n",
    "       \n",
    "        decoded_sentence[t,:] = output_tokens.copy()\n",
    "\n",
    "        # exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if t >= max_seq_length - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = output_tokens\n",
    "        t += 1\n",
    "\n",
    "    return decoded_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
