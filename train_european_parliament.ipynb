{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from lstm_vae import create_lstm_vae, inference\n",
    "import keras\n",
    "import sys, time\n",
    "from keras.callbacks import CSVLogger, Callback\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "def get_text_data(data_path, num_samples=1000):\n",
    "\n",
    "    # vectorize the data\n",
    "    input_texts = []\n",
    "    input_characters = set([\"\\t\"])\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        #lines = f.read().lower().split('\\n')\n",
    "        lines = f.read().lower().split('\\n')\n",
    "        \n",
    "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "\n",
    "        #input_text, _= line.split('\\t')\n",
    "        input_text = line\n",
    "        input_text = word_tokenize(input_text)\n",
    "        input_text.append(\"<end>\")\n",
    "\n",
    "        input_texts.append(input_text)\n",
    "\n",
    "        for char in input_text:\n",
    "            if char not in input_characters:\n",
    "                input_characters.add(char)\n",
    "\n",
    "    input_characters = sorted(list(input_characters))\n",
    "    num_encoder_tokens = len(input_characters)\n",
    "    max_encoder_seq_length = max([len(txt) for txt in input_texts]) + 1\n",
    "\n",
    "    print(\"Number of samples:\", len(input_texts))\n",
    "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "\n",
    "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "\n",
    "    encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
    "    decoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\")\n",
    "\n",
    "    for i, input_text in enumerate(input_texts):\n",
    "        decoder_input_data[i, 0, input_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "        for t, char in enumerate(input_text):\n",
    "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "            decoder_input_data[i, t + 1, input_token_index[char]] = 1.0\n",
    "\n",
    "    return max_encoder_seq_length, num_encoder_tokens, input_characters, input_token_index, reverse_input_char_index, \\\n",
    "           encoder_input_data, decoder_input_data\n",
    "\n",
    "\n",
    "def main(params):\n",
    "    \n",
    "    num_samples = int(params['num_samples'])\n",
    "    data_path = \"data/\" + params['dataset']\n",
    "    dataname = params['dataset'].split('.')[0]\n",
    "    \n",
    "    batch_size = int(params['batch_size'])\n",
    "    latent_dim = int(params['latent_dim'])\n",
    "    intermediate_dim = int(params['intermediate_dim'])\n",
    "    epochs = int(params['epochs'])\n",
    "    \n",
    "    train = int(params['train'])\n",
    "    save = int(params['save'])\n",
    "    load = int(params['load'])\n",
    "    \n",
    "    timesteps_max, enc_tokens, characters, char2id, id2char, x, x_decoder = get_text_data(num_samples=num_samples,\n",
    "                                                                                          data_path=data_path)\n",
    "\n",
    "    print(x.shape, \"Creating model...\")\n",
    "    \n",
    "    input_dim = x.shape[-1]\n",
    "    timesteps = x.shape[-2]\n",
    "    \n",
    "    if load:\n",
    "        print(\"Loading model ... \")\n",
    "        \n",
    "        #vae = keras.models.load_model(\"models/vae_{}.h5\".format(dataname))\n",
    "        enc = keras.models.load_model(\"models/encoder_{}.h5\".format(dataname))\n",
    "        gen = keras.models.load_model(\"models/generator_{}.h5\".format(dataname))\n",
    "        stepper = keras.models.load_model(\"models/stepper_{}.h5\".format(dataname))\n",
    "    \n",
    "    if train:\n",
    "        print(\"Training model...\")\n",
    "        \n",
    "        vae, enc, gen, stepper = create_lstm_vae(input_dim,\n",
    "                                             batch_size=batch_size,\n",
    "                                             intermediate_dim=intermediate_dim,\n",
    "                                             latent_dim=latent_dim)\n",
    "\n",
    "        \n",
    "        csv_logger = CSVLogger('training_vae.log', separator=',', append=False)\n",
    "        vae.fit([x, x_decoder], x, epochs=epochs, verbose=1, callbacks=[csv_logger])\n",
    "        \n",
    "        if save:\n",
    "            print(\"Saving model ... \")\n",
    "            \n",
    "            vae.save(\"models/vae_{}.h5\".format(dataname))\n",
    "            enc.save(\"models/encoder_{}.h5\".format(dataname))\n",
    "            gen.save(\"models/generator_{}.h5\".format(dataname))\n",
    "            stepper.save(\"models/stepper_{}.h5\".format(dataname))\n",
    "    \n",
    "    print(\"Fitted, predicting...\")\n",
    "\n",
    "\n",
    "    def decode(s, start_char = \"\\t\"):\n",
    "        return inference.decode_sequence(s, gen, stepper, input_dim, char2id, id2char, timesteps_max, start_char = start_char)\n",
    "\n",
    "    def continue_seq(x_start):\n",
    "        return inference.continue_sequence(x_start, gen, stepper, input_dim, char2id, id2char, timesteps_max)\n",
    "\n",
    "    for _ in range(5):\n",
    "\n",
    "        id_from = np.random.randint(0, x.shape[0] - 1)\n",
    "        id_to = np.random.randint(0, x.shape[0] - 1)\n",
    "\n",
    "        m_from, std_from = enc.predict([[x[id_from]]])\n",
    "        m_to, std_to = enc.predict([[x[id_to]]])\n",
    "\n",
    "        seq_from = np.random.normal(size=(latent_dim,))\n",
    "        seq_from = m_from + std_from * seq_from\n",
    "\n",
    "        seq_to = np.random.normal(size=(latent_dim,))\n",
    "        seq_to = m_to + std_to * seq_to\n",
    "\n",
    "        print(\"==  \\t\", \" \".join([id2char[j] for j in np.argmax(x[id_from], axis=1)]), \"==\")\n",
    "\n",
    "        for v in np.linspace(0, 1, 7):\n",
    "            print(\"%.2f\\t\" % (1 - v), decode(v * seq_to + (1 - v) * seq_from))\n",
    "\n",
    "        print(\"==  \\t\", \" \".join([id2char[j] for j in np.argmax(x[id_to], axis=1)]), \"==\")\n",
    "        \n",
    "    for _ in range(20):\n",
    "        id_sentence = np.random.randint(0, x.shape[0] - 1)\n",
    "        \n",
    "        n_words = np.sum(x[id_sentence])\n",
    "        n_kept = np.random.randint(n_words//2, n_words-1)\n",
    "        \n",
    "        new_x = np.zeros((x[id_sentence].shape))\n",
    "        new_x[:n_kept,:] = x[id_sentence,:n_kept,:]\n",
    "        \n",
    "        print(\"==  \\t\", \" \".join([id2char[j] for j in np.argmax(new_x[:n_kept], axis=1)]), \" ... \\t\\t ==\")\n",
    "        \n",
    "        print(\"\\t...\\t\", continue_seq(new_x))\n",
    "            \n",
    "        print(\"==  \\t\", \" \".join([id2char[j] for j in np.argmax(x[id_sentence], axis=1)]), \"==\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/europ.en' \n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        #lines = f.read().lower().split('\\n')\n",
    "        lines = f.read().lower().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100\n",
      "Number of unique input tokens: 750\n",
      "Max sequence length for inputs: 117\n",
      "(100, 117, 750) Creating model...\n",
      "Training model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, None, 750)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 353)          1558848     input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 191)          67614       lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 191)          67614       lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 191)          0           dense_17[0][0]                   \n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, None, 750)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 353)          67776       lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 353),  1558848     input_22[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 750)    265500      lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,586,200\n",
      "Trainable params: 3,586,200\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 1.6680 - xent_loss: 1.6680 - kl_loss: 4.4382e-06\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 1.6556 - xent_loss: 1.6556 - kl_loss: 4.0681e-05\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.5375 - xent_loss: 1.5370 - kl_loss: 4.7237e-04\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 1.4439 - xent_loss: 1.4013 - kl_loss: 0.0426\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 1.3918 - xent_loss: 1.3910 - kl_loss: 7.4288e-04\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 1.3800 - xent_loss: 1.3793 - kl_loss: 6.3353e-04\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.3717 - xent_loss: 1.3708 - kl_loss: 8.4109e-04\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.3660 - xent_loss: 1.3648 - kl_loss: 0.0012\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 1.3660 - xent_loss: 1.3643 - kl_loss: 0.0017\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 1.3699 - xent_loss: 1.3680 - kl_loss: 0.0020\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 1.3666 - xent_loss: 1.3648 - kl_loss: 0.0018\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.3639 - xent_loss: 1.3623 - kl_loss: 0.0016\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 1.3599 - xent_loss: 1.3586 - kl_loss: 0.0014\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 1.3571 - xent_loss: 1.3559 - kl_loss: 0.0012\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.3537 - xent_loss: 1.3525 - kl_loss: 0.0011\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.3512 - xent_loss: 1.3501 - kl_loss: 0.0010\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 1.3487 - xent_loss: 1.3477 - kl_loss: 9.5599e-04\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.3471 - xent_loss: 1.3462 - kl_loss: 9.5037e-04\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 1.3459 - xent_loss: 1.3449 - kl_loss: 9.9048e-04\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 1.3423 - xent_loss: 1.3412 - kl_loss: 0.0011\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.3390 - xent_loss: 1.3378 - kl_loss: 0.0011\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.3351 - xent_loss: 1.3339 - kl_loss: 0.0012\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.3313 - xent_loss: 1.3300 - kl_loss: 0.0013\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 1.3285 - xent_loss: 1.3270 - kl_loss: 0.0014\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.3255 - xent_loss: 1.3240 - kl_loss: 0.0015\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.3205 - xent_loss: 1.3189 - kl_loss: 0.0016\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.3180 - xent_loss: 1.3164 - kl_loss: 0.0016\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 1.3156 - xent_loss: 1.3141 - kl_loss: 0.0015\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 1.3112 - xent_loss: 1.3097 - kl_loss: 0.0016\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 1.3059 - xent_loss: 1.3041 - kl_loss: 0.0017\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.3033 - xent_loss: 1.3017 - kl_loss: 0.0016\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 1.3001 - xent_loss: 1.2984 - kl_loss: 0.0016\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.2957 - xent_loss: 1.2941 - kl_loss: 0.0016\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 1.2916 - xent_loss: 1.2901 - kl_loss: 0.0015\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 1.2885 - xent_loss: 1.2872 - kl_loss: 0.0013\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.2845 - xent_loss: 1.2833 - kl_loss: 0.0013\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 1.2814 - xent_loss: 1.2802 - kl_loss: 0.0012\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.2783 - xent_loss: 1.2771 - kl_loss: 0.0013\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 1.2757 - xent_loss: 1.2743 - kl_loss: 0.0013\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.2731 - xent_loss: 1.2717 - kl_loss: 0.0014\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.2687 - xent_loss: 1.2674 - kl_loss: 0.0013\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 10s 105ms/step - loss: 1.2669 - xent_loss: 1.2656 - kl_loss: 0.0012\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.2649 - xent_loss: 1.2637 - kl_loss: 0.0012\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 1.2610 - xent_loss: 1.2598 - kl_loss: 0.0012\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 11s 109ms/step - loss: 1.2574 - xent_loss: 1.2562 - kl_loss: 0.0012\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.2545 - xent_loss: 1.2534 - kl_loss: 0.0012\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.2532 - xent_loss: 1.2519 - kl_loss: 0.0013\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.2491 - xent_loss: 1.2480 - kl_loss: 0.0011\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 1.2451 - xent_loss: 1.2439 - kl_loss: 0.0012\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.2411 - xent_loss: 1.2399 - kl_loss: 0.0011\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 1.2391 - xent_loss: 1.2379 - kl_loss: 0.0012\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.2368 - xent_loss: 1.2358 - kl_loss: 0.0010\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 1.2322 - xent_loss: 1.2310 - kl_loss: 0.0012\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.2283 - xent_loss: 1.2271 - kl_loss: 0.0012\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 1.2241 - xent_loss: 1.2229 - kl_loss: 0.0012\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 1.2208 - xent_loss: 1.2196 - kl_loss: 0.0012\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 1.2174 - xent_loss: 1.2163 - kl_loss: 0.0011\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.2150 - xent_loss: 1.2139 - kl_loss: 0.0011\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 1.2105 - xent_loss: 1.2094 - kl_loss: 0.0011\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 1.2065 - xent_loss: 1.2055 - kl_loss: 0.0010\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.2026 - xent_loss: 1.2015 - kl_loss: 0.0010\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 1.1991 - xent_loss: 1.1981 - kl_loss: 0.0010\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 1.1948 - xent_loss: 1.1938 - kl_loss: 9.8644e-04\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 1.1890 - xent_loss: 1.1880 - kl_loss: 9.7658e-04\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.1864 - xent_loss: 1.1854 - kl_loss: 9.6240e-04\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 1.1819 - xent_loss: 1.1810 - kl_loss: 9.4234e-04\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.1812 - xent_loss: 1.1803 - kl_loss: 8.7288e-04\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.1756 - xent_loss: 1.1747 - kl_loss: 9.1162e-04\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.1717 - xent_loss: 1.1707 - kl_loss: 9.7695e-04\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.1698 - xent_loss: 1.1689 - kl_loss: 9.8957e-04\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 1.1652 - xent_loss: 1.1642 - kl_loss: 0.0010\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 1.1586 - xent_loss: 1.1577 - kl_loss: 9.2989e-04\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 1.1550 - xent_loss: 1.1541 - kl_loss: 8.6328e-04\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.1501 - xent_loss: 1.1492 - kl_loss: 8.6785e-04\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 1.1446 - xent_loss: 1.1437 - kl_loss: 8.3821e-04\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 1.1372 - xent_loss: 1.1364 - kl_loss: 8.6879e-04\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 1.1364 - xent_loss: 1.1355 - kl_loss: 8.9167e-04\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 1.1311 - xent_loss: 1.1303 - kl_loss: 8.2224e-04\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.1289 - xent_loss: 1.1280 - kl_loss: 8.8157e-04\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.1251 - xent_loss: 1.1242 - kl_loss: 8.6548e-04\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.1164 - xent_loss: 1.1154 - kl_loss: 9.1685e-04\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.1109 - xent_loss: 1.1099 - kl_loss: 9.7997e-04\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.1047 - xent_loss: 1.1037 - kl_loss: 9.7510e-04\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.0991 - xent_loss: 1.0981 - kl_loss: 9.6620e-04\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.0941 - xent_loss: 1.0932 - kl_loss: 9.2247e-04\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 1.0870 - xent_loss: 1.0862 - kl_loss: 8.7140e-04\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 1.0816 - xent_loss: 1.0808 - kl_loss: 8.3967e-04\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 1.0761 - xent_loss: 1.0753 - kl_loss: 7.7741e-04\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 1.0726 - xent_loss: 1.0718 - kl_loss: 8.1328e-04\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.0645 - xent_loss: 1.0637 - kl_loss: 7.9798e-04\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 1.0591 - xent_loss: 1.0583 - kl_loss: 7.7410e-04\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.0522 - xent_loss: 1.0515 - kl_loss: 7.5369e-04\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 1.0486 - xent_loss: 1.0479 - kl_loss: 7.3306e-04\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 1.0402 - xent_loss: 1.0395 - kl_loss: 7.3844e-04\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 1.0377 - xent_loss: 1.0370 - kl_loss: 7.1246e-04\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 1.0301 - xent_loss: 1.0294 - kl_loss: 7.1245e-04\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 1.0238 - xent_loss: 1.0230 - kl_loss: 7.3527e-04\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 1.0160 - xent_loss: 1.0153 - kl_loss: 7.0405e-04\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.0119 - xent_loss: 1.0112 - kl_loss: 7.5393e-04\n",
      "Epoch 100/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 1.0061 - xent_loss: 1.0054 - kl_loss: 7.5584e-04\n",
      "Epoch 101/200\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.9984 - xent_loss: 0.9977 - kl_loss: 7.6427e-04\n",
      "Epoch 102/200\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.9954 - xent_loss: 0.9946 - kl_loss: 7.5381e-04\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.9924 - xent_loss: 0.9916 - kl_loss: 7.2689e-04\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.9836 - xent_loss: 0.9829 - kl_loss: 6.9106e-04\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.9757 - xent_loss: 0.9750 - kl_loss: 6.9991e-04\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - 12s 115ms/step - loss: 0.9687 - xent_loss: 0.9680 - kl_loss: 6.8175e-04\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.9619 - xent_loss: 0.9613 - kl_loss: 6.5281e-04\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.9545 - xent_loss: 0.9538 - kl_loss: 6.8288e-04\n",
      "Epoch 109/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 11s 113ms/step - loss: 0.9492 - xent_loss: 0.9485 - kl_loss: 6.9643e-04\n",
      "Epoch 110/200\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.9410 - xent_loss: 0.9403 - kl_loss: 6.9243e-04\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.9362 - xent_loss: 0.9355 - kl_loss: 6.3446e-04\n",
      "Epoch 112/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.9284 - xent_loss: 0.9278 - kl_loss: 6.1698e-04\n",
      "Epoch 113/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.9229 - xent_loss: 0.9223 - kl_loss: 6.2052e-04\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.9171 - xent_loss: 0.9165 - kl_loss: 6.0840e-04\n",
      "Epoch 115/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.9101 - xent_loss: 0.9095 - kl_loss: 6.1723e-04\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.9076 - xent_loss: 0.9069 - kl_loss: 6.4177e-04\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - 10s 105ms/step - loss: 0.9013 - xent_loss: 0.9006 - kl_loss: 6.7571e-04\n",
      "Epoch 118/200\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.8967 - xent_loss: 0.8960 - kl_loss: 6.6677e-04\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.9375 - xent_loss: 0.9369 - kl_loss: 6.2895e-04\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.8808 - xent_loss: 0.8801 - kl_loss: 6.5241e-04\n",
      "Epoch 121/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.8754 - xent_loss: 0.8747 - kl_loss: 6.6309e-04\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.8717 - xent_loss: 0.8711 - kl_loss: 5.8315e-04\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.8609 - xent_loss: 0.8604 - kl_loss: 5.5426e-04\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.8552 - xent_loss: 0.8546 - kl_loss: 6.0011e-04\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 0.8469 - xent_loss: 0.8463 - kl_loss: 6.5099e-04\n",
      "Epoch 126/200\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 0.8406 - xent_loss: 0.8400 - kl_loss: 6.5773e-04\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.8326 - xent_loss: 0.8319 - kl_loss: 6.3867e-04\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.8261 - xent_loss: 0.8255 - kl_loss: 6.0190e-04\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.8187 - xent_loss: 0.8181 - kl_loss: 5.9442e-04\n",
      "Epoch 130/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.8121 - xent_loss: 0.8115 - kl_loss: 5.8734e-04\n",
      "Epoch 131/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.8088 - xent_loss: 0.8083 - kl_loss: 5.7916e-04\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.8006 - xent_loss: 0.8000 - kl_loss: 5.7091e-04\n",
      "Epoch 133/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.7959 - xent_loss: 0.7953 - kl_loss: 5.4053e-04\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.7883 - xent_loss: 0.7878 - kl_loss: 4.9571e-04\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.7816 - xent_loss: 0.7811 - kl_loss: 4.7171e-04\n",
      "Epoch 136/200\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.7768 - xent_loss: 0.7763 - kl_loss: 4.8811e-04\n",
      "Epoch 137/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.7701 - xent_loss: 0.7696 - kl_loss: 5.1553e-04\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.7649 - xent_loss: 0.7643 - kl_loss: 5.2605e-04\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.7575 - xent_loss: 0.7570 - kl_loss: 5.2125e-04\n",
      "Epoch 140/200\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.7482 - xent_loss: 0.7477 - kl_loss: 4.9553e-04\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 0.7426 - xent_loss: 0.7421 - kl_loss: 4.8884e-04\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.7355 - xent_loss: 0.7350 - kl_loss: 4.9192e-04\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.7329 - xent_loss: 0.7323 - kl_loss: 5.1791e-04\n",
      "Epoch 144/200\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.7259 - xent_loss: 0.7254 - kl_loss: 5.4291e-04\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.7190 - xent_loss: 0.7184 - kl_loss: 5.5783e-04\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.7129 - xent_loss: 0.7123 - kl_loss: 5.6628e-04\n",
      "Epoch 147/200\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.7093 - xent_loss: 0.7087 - kl_loss: 5.8165e-04\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.6980 - xent_loss: 0.6974 - kl_loss: 5.8665e-04\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.6888 - xent_loss: 0.6883 - kl_loss: 5.6665e-04\n",
      "Epoch 150/200\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.6824 - xent_loss: 0.6819 - kl_loss: 5.5413e-04\n",
      "Epoch 151/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.6788 - xent_loss: 0.6782 - kl_loss: 5.5867e-04\n",
      "Epoch 152/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.6710 - xent_loss: 0.6705 - kl_loss: 5.4706e-04\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.6664 - xent_loss: 0.6658 - kl_loss: 5.3458e-04\n",
      "Epoch 154/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.6594 - xent_loss: 0.6588 - kl_loss: 5.2743e-04\n",
      "Epoch 155/200\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.6583 - xent_loss: 0.6578 - kl_loss: 5.0491e-04\n",
      "Epoch 156/200\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 0.6520 - xent_loss: 0.6515 - kl_loss: 4.8994e-04\n",
      "Epoch 157/200\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 0.6497 - xent_loss: 0.6492 - kl_loss: 5.1961e-04\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.6399 - xent_loss: 0.6393 - kl_loss: 5.9835e-04\n",
      "Epoch 159/200\n",
      "100/100 [==============================] - 12s 121ms/step - loss: 0.6284 - xent_loss: 0.6278 - kl_loss: 6.2606e-04\n",
      "Epoch 160/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.6212 - xent_loss: 0.6206 - kl_loss: 6.2665e-04\n",
      "Epoch 161/200\n",
      "100/100 [==============================] - 12s 116ms/step - loss: 0.6104 - xent_loss: 0.6097 - kl_loss: 6.1798e-04\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - 12s 122ms/step - loss: 0.6057 - xent_loss: 0.6052 - kl_loss: 5.4767e-04\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.6002 - xent_loss: 0.5996 - kl_loss: 5.3432e-04\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.6089 - xent_loss: 0.6084 - kl_loss: 5.0412e-04\n",
      "Epoch 165/200\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.5999 - xent_loss: 0.5994 - kl_loss: 5.1386e-04\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.5905 - xent_loss: 0.5899 - kl_loss: 5.5042e-04\n",
      "Epoch 167/200\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.5790 - xent_loss: 0.5784 - kl_loss: 6.0184e-04\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 0.5704 - xent_loss: 0.5698 - kl_loss: 5.9411e-04\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - 10s 102ms/step - loss: 0.5619 - xent_loss: 0.5613 - kl_loss: 5.6352e-04\n",
      "Epoch 170/200\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.5536 - xent_loss: 0.5530 - kl_loss: 6.0305e-04\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.5447 - xent_loss: 0.5441 - kl_loss: 5.8917e-04\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 10s 99ms/step - loss: 0.5369 - xent_loss: 0.5364 - kl_loss: 5.5639e-04\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.5347 - xent_loss: 0.5341 - kl_loss: 5.6583e-04\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.5356 - xent_loss: 0.5350 - kl_loss: 5.7903e-04\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.5288 - xent_loss: 0.5283 - kl_loss: 5.6981e-04\n",
      "Epoch 176/200\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.5197 - xent_loss: 0.5191 - kl_loss: 5.4943e-04\n",
      "Epoch 177/200\n",
      "100/100 [==============================] - 11s 115ms/step - loss: 0.5122 - xent_loss: 0.5117 - kl_loss: 5.1559e-04\n",
      "Epoch 178/200\n",
      "100/100 [==============================] - 11s 114ms/step - loss: 0.5070 - xent_loss: 0.5065 - kl_loss: 4.8892e-04\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.5020 - xent_loss: 0.5015 - kl_loss: 4.7457e-04\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.4971 - xent_loss: 0.4966 - kl_loss: 4.5637e-04\n",
      "Epoch 181/200\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 0.4883 - xent_loss: 0.4878 - kl_loss: 4.4777e-04\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - 11s 113ms/step - loss: 0.4851 - xent_loss: 0.4847 - kl_loss: 4.4495e-04\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - 10s 103ms/step - loss: 0.4768 - xent_loss: 0.4764 - kl_loss: 4.4491e-04\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.4667 - xent_loss: 0.4663 - kl_loss: 4.5222e-04\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.4584 - xent_loss: 0.4579 - kl_loss: 4.4927e-04\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - 11s 105ms/step - loss: 0.4504 - xent_loss: 0.4500 - kl_loss: 4.3871e-04\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.4438 - xent_loss: 0.4434 - kl_loss: 4.5213e-04\n",
      "Epoch 188/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.4374 - xent_loss: 0.4370 - kl_loss: 4.7341e-04\n",
      "Epoch 189/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.4320 - xent_loss: 0.4315 - kl_loss: 4.8816e-04\n",
      "Epoch 190/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.4297 - xent_loss: 0.4292 - kl_loss: 4.9113e-04\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.4295 - xent_loss: 0.4291 - kl_loss: 4.8545e-04\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - 11s 111ms/step - loss: 0.4387 - xent_loss: 0.4382 - kl_loss: 5.0393e-04\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.4317 - xent_loss: 0.4311 - kl_loss: 5.2244e-04\n",
      "Epoch 194/200\n",
      "100/100 [==============================] - 10s 104ms/step - loss: 0.4299 - xent_loss: 0.4295 - kl_loss: 4.9823e-04\n",
      "Epoch 195/200\n",
      "100/100 [==============================] - 11s 107ms/step - loss: 0.4184 - xent_loss: 0.4179 - kl_loss: 4.9598e-04\n",
      "Epoch 196/200\n",
      "100/100 [==============================] - 11s 106ms/step - loss: 0.4193 - xent_loss: 0.4188 - kl_loss: 5.2391e-04\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.4094 - xent_loss: 0.4089 - kl_loss: 5.4099e-04\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - 11s 110ms/step - loss: 0.3989 - xent_loss: 0.3983 - kl_loss: 5.8417e-04\n",
      "Epoch 199/200\n",
      "100/100 [==============================] - 11s 112ms/step - loss: 0.3897 - xent_loss: 0.3891 - kl_loss: 6.1765e-04\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - 11s 109ms/step - loss: 0.3824 - xent_loss: 0.3817 - kl_loss: 6.5020e-04\n",
      "Saving model ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/axeldelavergne/anaconda3/envs/first_env/lib/python3.5/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'dense_19/BiasAdd:0' shape=(?, 353) dtype=float32>, <tf.Tensor 'dense_19/BiasAdd:0' shape=(?, 353) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/axeldelavergne/anaconda3/envs/first_env/lib/python3.5/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'dense_19_1/BiasAdd:0' shape=(?, 353) dtype=float32>, <tf.Tensor 'dense_19_1/BiasAdd:0' shape=(?, 353) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/axeldelavergne/anaconda3/envs/first_env/lib/python3.5/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_24:0' shape=(?, 353) dtype=float32>, <tf.Tensor 'input_25:0' shape=(?, 353) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted, predicting...\n",
      "==  \t but , madam president , my personal request has not been met . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "1.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.83\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.67\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.50\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.33\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.17\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "==  \t the cunha report on multiannual guidance programmes comes before parliament on thursday and contains a proposal in paragraph 6 that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t but i would also like to make it very clear that president prodi made a commitment to this parliament to introduce a new debate , as mr barón crespo has reminded us , which would be in addition to the annual debate on the commission ' s legislative programme , on the broad areas of action for the next five years , that is to say , for this legislature . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "1.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.83\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.67\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.50\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.33\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.17\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "==  \t madam president , on a point of order . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t it is the case of alexander nikitin . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "1.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.83\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.67\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.50\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.33\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.17\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "==  \t why has there been no health and safety committee meeting since 1998 ? <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t i shall also refer the matter to the college of quaestors , and i am certain that they will be keen to ensure that we comply with the regulations we ourselves vote on . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "1.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.83\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.67\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.50\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.33\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.17\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "==  \t mrs plooij-van gorsel , i can tell you that this matter is on the agenda for the quaestors ' meeting on wednesday . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t is there a member who wishes to speak on behalf of this group to propose this ? <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "1.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.83\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.67\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.50\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.33\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.17\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "0.00\t i would like to advice about rule 143 concerning inadmissibility . <end> \n",
      "==  \t yes , mr evans , i feel an initiative of the type you have just suggested would be entirely appropriate . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t the questions answered previously referred to mrs de palacio ' s intervention , on another occasion , and not to these comments which appeared in the abc newspaper on 18  ... \t\t ==\n",
      "\t...\t programme . <end> \n",
      "==  \t the questions answered previously referred to mrs de palacio ' s intervention , on another occasion , and not to these comments which appeared in the abc newspaper on 18 november . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t thank you , mr segni , i  ... \t\t ==\n",
      "\t...\t would be to do so . <end> \n",
      "==  \t thank you , mr segni , i shall do so gladly . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t in the meantime , i should like to observe a minute ' s silence , as a number of members have requested , on behalf of all the victims concerned , particularly those of the terrible storms , in the various countries of the  ... \t\t ==\n",
      "\t...\t european community , on the commission of our and , the next of the commission ' s legislative programme , on the broad \n",
      "==  \t in the meantime , i should like to observe a minute ' s silence , as a number of members have requested , on behalf of all the victims concerned , particularly those of the terrible storms , in the various countries of the european union . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t why are no-smoking areas  ... \t\t ==\n",
      "\t...\t to the and of the next five years , that this parliament , to this parliament , as it is be . <end> \n",
      "==  \t why are no-smoking areas not enforced ? <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t would it be appropriate for you , madam president , to write a letter to the sri lankan president expressing parliament 's regret at his and the other violent deaths in sri lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult  ... \t\t ==\n",
      "\t...\t . <end> \n",
      "==  \t would it be appropriate for you , madam president , to write a letter to the sri lankan president expressing parliament 's regret at his and the other violent deaths in sri lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation ? <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t given that the commission is represented by vice-president de palacio , i believe that , before voting , it would help if the commission could let us know how ready it is to present this programme , as agreed . alternatively , parliament is not ready to examine this programme , as some appear to be suggesting  ... \t\t ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t...\t . <end> \n",
      "==  \t given that the commission is represented by vice-president de palacio , i believe that , before voting , it would help if the commission could let us know how ready it is to present this programme , as agreed . alternatively , parliament is not ready to examine this programme , as some appear to be suggesting . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t but , madam president , my personal request has not been met  ... \t\t ==\n",
      "\t...\t . <end> \n",
      "==  \t but , madam president , my personal request has not been met . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t therefore , madam president , i would ask you to request that the commission express its opinion  ... \t\t ==\n",
      "\t...\t , we had agreed - , the commission is not a debate on the next five years and that we have to to the embargo . <end> \n",
      "==  \t therefore , madam president , i would ask you to request that the commission express its opinion on this issue and that we then proceed to the vote . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t at the request of a french member , mr zimeray , a petition has already been presented , which  ... \t\t ==\n",
      "\t...\t in the speech . <end> \n",
      "==  \t at the request of a french member , mr zimeray , a petition has already been presented , which many people signed , including myself . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t madam president , on a point  ... \t\t ==\n",
      "\t...\t of order . <end> \n",
      "==  \t madam president , on a point of order . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t i would ask that they reconsider , since this  ... \t\t ==\n",
      "\t...\t parliament is not ready to do so , and , we is not to do so . <end> \n",
      "==  \t i would ask that they reconsider , since this is not the case . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t in any event , this question is not presently included among the requests for topical and urgent  ... \t\t ==\n",
      "\t...\t for the next five years and that we have to to the programme . <end> \n",
      "==  \t in any event , this question is not presently included among the requests for topical and urgent debate on thursday . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t we do not know what is happening  ... \t\t ==\n",
      "\t...\t . <end> \n",
      "==  \t we do not know what is happening . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t mrs plooij-van gorsel , i can tell you that this matter is on the agenda for  ... \t\t ==\n",
      "\t...\t the next five years and that we have proceed to the embargo . <end> \n",
      "==  \t mrs plooij-van gorsel , i can tell you that this matter is on the agenda for the quaestors ' meeting on wednesday . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t why are no-smoking areas not enforced  ... \t\t ==\n",
      "\t...\t ? <end> \n",
      "==  \t why are no-smoking areas not enforced ? <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t no amendments have been proposed relating to monday  ... \t\t ==\n",
      "\t...\t and . <end> \n",
      "==  \t no amendments have been proposed relating to monday and tuesday . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t as people have said , the situation  ... \t\t ==\n",
      "\t...\t of the next five years , the next parliament , , in , in the speech in a speech . <end> \n",
      "==  \t as people have said , the situation there is extremely volatile . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed  ... \t\t ==\n",
      "\t...\t a speech . <end> \n",
      "==  \t i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t why are there no fire  ... \t\t ==\n",
      "\t...\t , and , in the prodi , which would be presented , as this parliament has on this week and on the broad areas of on and \n",
      "==  \t why are there no fire instructions ? <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n",
      "==  \t my group believes that since a parliament is meant to listen , debate and reflect , there can be no justification whatsoever for this delay and we believe that , if the commission is ready to do so , we still have time to re-establish the original agreement between parliament and the commission and proceed in a  ... \t\t ==\n",
      "\t...\t manner which fulfils our citizens . <end> \n",
      "==  \t my group believes that since a parliament is meant to listen , debate and reflect , there can be no justification whatsoever for this delay and we believe that , if the commission is ready to do so , we still have time to re-establish the original agreement between parliament and the commission and proceed in a manner which fulfils our duty to our fellow citizens . <end> \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t ==\n"
     ]
    }
   ],
   "source": [
    "config_file_name = 'config.txt'\n",
    "\n",
    "with open(config_file_name, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "params = {}\n",
    "\n",
    "for line in lines:\n",
    "    line = line.split('\\n')[0]\n",
    "    param_list = line.split(' ')\n",
    "    param_name = param_list[0]\n",
    "    param_value = param_list[1]\n",
    "    params[param_name] = param_value\n",
    "\n",
    "main(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
